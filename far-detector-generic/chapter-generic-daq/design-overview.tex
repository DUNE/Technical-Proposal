%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}
\label{sec:fd-daq-ltr}

\metainfo{Giles Barr, BV. 
  This is a shared DP/SP section. 
  We try to describe the high level design from
  Figure~\ref{fig:daq-overview} in more detail than in the main
  overview section but defer to the coming sections for ``inside the
  boxes'' detail as well as to break make distinct SP and DP
  implementation in their respective tex files.}


The design for the \dword{daq} has been driven by finding a cost
effective solution that satisfies the requirements. Several design
choices have nevertheless been made. 
From a hardware perspecive, the DAQ design follows a standard HEP
experiment design, with customised hardware at the upstream, feeding
and funnelling (merging) and moving the data into computers. 
Once the data and triggering information are in computers, a
considerable degree of flexibility is available and the processing
proceeds with a pipelined sequence of software operations, involving
both parallel processing on multi-core computers and switched
networks. The flexibility allows the procurement of computers and
networking to be done late in the delivery cycle of the DUNE
detectors, to benefit from increased capability of commercial devices
and falling prices.

Since DUNE will operate over a number of decades, the DAQ has been
designed with upgradability in mind.  With the fall in cost of serial
links, a guiding principle is to include enough output bandwidth to
allow all the data to be passed downstream of the custom hardware.
This allows the possibility for a future very-fast farm of computing
elements to accommodate new ideas in how to collect the DUNE data.  The
high output bandwidth also gives a risk mitigation path in case the
noise levels in a part of the detector are higher than specified and
higher than tolerable by the baseline trigger decision mechanism; it
will allow additional data processing to be added (at additional cost).

Digital data will be collected from the TPC and photon-detector
readout electronics of the single-phase and dual-phase
\dwords{detmodule}. 
These categories of data sources are viewed as essentially four types
of \dwords{submodule} within the DAQ and will follow the same overall
data collection scheme as shown in Figure~\ref{fig:daq-overview}. 
The readout is arranged to allow for a \dword{trigdecision} to be made
in a hierarchical manner. 
Initial inputs are formed at the channel level, then combined at the
\dword{detunit} level and likewise these are combined at the
\dword{detmodule} level.
In addition, the \dword{trigdecision} process combines this level of
information that may come from the other \dwords{detmodule} as well as
information sources external to the DAQ. 
The control of the detector elements will prevent a fault in one part
from halting data taking in another part, certainly at the per-cavern
level, and potentially also at more local levels. 
This will allow a supernova burst to be recorded in all operational
parts of the detector.

It is helpful, on first looking at the DUNE DAQ dataflow scheme, to
consider how the main data is collected first and then to look at how
the triggering and synchronization functions are included; we take
this approach in this overview. 

A single \dword{daqfrag} associates an integral number of
\dwords{detunit} with one \dfirst{fec}. 
Limited largely by PCIe bandwidth a \dword{fec} may accept
10-20~\si{\GB/\s}. 
This allows one \dword{sp} fragment to correspond to two APAs and one
\dword{dp} fragment to correspond to about ten \dfirst{cro} crates or
possibly all five \dfirst{lro} crates.
The data of one fragment first enters the DAQ via the fragment's
\dfirst{daqfer} component.
In the \dword{sp} the \dword{daqfer} consists of eight \dwords{rce} and
in the \dword{dp} it consists of a number of computers.  
The \dword{daqfer} is responsible for accepting that data and from it
producing channel level \dwords{trigprimitive}.
It is also responsible for forwarding compressed data and the
primitives to the \dfirst{daqdr} in the corresponding \dword{fec}.

In the nominal design the \dword{daqfer} is also responsible for
supplying transient (RAM) and non-volatile buffer in the form of
\dword{ssd} sufficient for \dword{snb} triggering and readout.
The \dword{daqdr} accepts the full data stream and transfers it to the
\dlong{daqbuf} of its \dword{daqfrag}. 
There it is held awaiting a query from the \dfirst{eb}. 
When the \dword{eb} receives a \dword{trigcommand} it uses the
included information to query all appropriate \dwords{daqds} and from
their returned \dwords{datafrag} an \dword{rawevent} is built and
written to file on the \dword{diskbuffer}. 
From there the data becomes responsibility of the Offline group to
transfer to Fermilab for permanent storage and further processing.

Meanwhile to the above the \dwords{trigprimitive} that were produced
by a \dword{daqfer} and sent to the \dword{daqdr} are used to form
\dwords{trigcandidate}. 
The \dwords{trigcandidate} emitted by all \dwords{daqfrag} are
collected in the \dfirst{mtl} associated with the \dword{daqpart}. 
There, they are time ordered and otherwise processed to form
\dwords{trigcommand}. 
Each such command is consumed by an \dword{eb} in order to query back
to the \dword{daqpart} level as described above.
In addition, the \dword{mtl} of one module is exchanging messages
in the form of \dwords{trigcandidate} with the others. 
For example, one module may raise a local \dword{snb}
\dword{trigcandidate} and forward it to all other modules.
Each module is also emitting candidates to sinks and accepting them
from sources of external trigger information.

The implementation of some of these high level functions depends on
particular \dword{detmodule} and different options exist for some
functions.  These are issues are described in the following sections.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overview}
\label{sec:fd-daq-overview}

\metainfo{Giles Barr, BV. 
  This is a shared DP/SP section. 
  We try to describe the high level design from
  Figure~\ref{fig:daq-overview} in more detail than in the main
  overview section but defer to the coming sections for ``inside the
  boxes'' detail as well as to break make distinct SP and DP
  implementation in their respective tex files.}


The design for the \dword{daq} has been driven by finding a cost
effective solution that satisfies the requirements. Several design
choices have nevertheless been made and two major variations remain to
be studied. 
From a hardware perspecive, the DAQ design follows a standard HEP
experiment design, with customised hardware at the upstream, feeding
and funnelling (merging) and moving the data into computers. 
Once the data and triggering information are in computers, a
considerable degree of flexibility is available and the processing
proceeds with a pipelined sequence of software operations, involving
both parallel processing on multi-core computers and switched
networks. The flexibility allows the procurement of computers and
networking to be done late in the delivery cycle of the DUNE
detectors, to benefit from increased capability of commercial devices
and falling prices.

Since DUNE will operate over a number of decades, the DAQ has been
designed with upgradability in mind.  With the fall in cost of serial
links, a guiding principle is to include enough output bandwidth to
allow all the data to be passed downstream of the custom hardware.
This allows the possibility for a future very-fast farm of computing
elements to accommodate new ideas in how to collect the DUNE data.  The
high output bandwidth also gives a risk mitigation path in case the
noise levels in a part of the detector are higher than specified and
higher than tolerable by the baseline trigger decision mechanism; it
will allow additional data processing to be added (at additional cost).

Digital data will be collected from the TPC and photon-detector
readout electronics of the single-phase and dual-phase
\dwords{detmodule}. 
These categories of data sources are viewed as essentially four types
of \dwords{submodule} within the DAQ and will follow the same overall
data collection scheme as shown for the nominal design in
Figure~\ref{fig:daq-overview} and for the alternative design in
Figure~\ref{fig:daq-overview-alt}. 
The readout is arranged to allow for a \dword{trigdecision} to be made
in a hierarchical manner. 
Initial inputs are formed at the channel level, then combined at the
\dword{detunit} level and likewise these are combined at the
\dword{detmodule} level.
In addition, the \dword{trigdecision} process combines this level of
information that may come from the other \dwords{detmodule} as well as
information sources external to the DAQ. 
This hierarchical structure in forming and consuming triggers will
allow safeguards to be developed so that any problems in one cavern or
in one \dword{detunit} of one \dword{detmodule} need not overwhelm the
entire DAQ.
It will also allow a supernova neutrino burst to be recorded in all
operational parts of the detector while others may be down for
calibration or maintenance.

Generally speaking, the DAQ consists of data and trigger flow.
The trigger flow involved in self-triggering originates from
processing a portion of the data flow. 
The trigger flow is then consumed back by the DAQ in order to govern
what portion of the data flow is finally written out to permanent
storage. 
The nominal and alternative designs differ in where in the data flow
the trigger flow originates. 

In both designs, a single \dword{daqfrag} associates an integral
number of \dwords{detunit} with one \dfirst{fec}.
This fragment forms one conceptual unit of the front-end DAQ.
The processing on a \dword{fec} is kept minimal such that each has a
throughput limited by I/O bandwidth. 
PCIe v4 doubles the bandwidth from the prior version and thus we
assume $\approx$\SI{20}{\GB/\s} throughput (out of a theoretical
\SI{32}{\GB/\s} max) can be achieved based on tests using PCIe v3.

In principle then, this allows one \dword{fec} to accept the data from
two (if uncompressed) or ten (if $5\times$ compressed) of the 150
\dword{sp} APAs, the $10\times$ compressed data from ten of the 240
\dword{dp} \dword{cro} crates or the uncompressed data from all five
\dword{dp} \dword{lro} crates.

In the nominal design, the data enters the DAQ via the fragment's
\dfirst{daqfer} component.
In the \dword{sp} the \dword{daqfer} consists of eight \dwords{rce}
and in the \dword{dp} it consists of a number of computers.  
The \dword{daqfer} is responsible for accepting that data and from it
producing channel level \dwords{trigprimitive}.
It is also responsible for forwarding compressed data and the
primitives to the \dfirst{daqdr} in the corresponding \dword{fec}.
The \dword{daqfer} is also responsible for supplying transient (RAM)
and non-volatile buffer in the form of \dword{ssd} sufficient for
\dword{snb} triggering and readout.
The \dword{daqdr} accepts the full data stream and transfers it to the
\dlong{daqbuf} of its \dword{daqfrag}. 
There it is held awaiting a query from the \dfirst{eb}. 
When the \dword{eb} receives a \dword{trigcommand} it uses the
included information to query all appropriate \dwords{daqds} and from
their returned \dwords{datafrag} an \dword{rawevent} is built and
written to file on the \dword{diskbuffer}. 
From there the data becomes responsibility of the Offline group to
transfer to Fermilab for permanent storage and further processing.

In the alternative design, the data is received directly by the
\dword{daqdr} in a \dword{fec} from the detector electronics:
\dword{sp} \dword{wib} or \dword{dp} \dword{utca} crate. 
The data then flows into the \dword{daqbuf} and the portion required
for forming trigger primitives is dispatched to the trigger computers
of the fragment for the production of \dwords{trigprimitive}.
At current SSD technology may allow SSD to be directly mounted to the
\dword{fec} to provide for the SNB dump buffer. 
Another solution which puts less pressure on write throughput is to
distribute the SSD to the trigger computers. 
In order to supply enough CPU they will likely have to number at least
two for every \dword{fec} and while their CPUs may be busy, their I/O
bandwidth will be relatively empty and thus they provide synergistic,
cost effective hosting for the SSDs.

Regardless of where the \dwords{trigprimitive} are produced in either
design they are further processed at the \dword{daqfrag} level to
produce \dwords{trigcandidate}. 
At this level, they represent possible activity localized in time and
channel to a portion of the overall \dword{detmodule}.
The \dwords{trigcandidate} emitted by all \dwords{daqfrag} are sent to
the \dfirst{mtl} associated with the \dword{daqpart}. 
There, they are time ordered and otherwise processed to form
\dwords{trigcommand}.
At this level they represent activity localized across the
\dword{detmodule} and over some period of time.

Each such command is consumed by an \dword{eb} in order to query back
to the \dword{daqpart} level as described above.
In addition, the \dword{mtl} of one module is exchanging messages
in the form of \dwords{trigcandidate} with the others. 
For example, one module may raise a local \dword{snb}
\dword{trigcandidate} and forward it to all other modules.
Each module is also emitting candidates to sinks and accepting them
from sources of external trigger information.

The exact implementation of some of these high level functions,
particularly those near the front-end, depends on particular
\dword{detmodule}. 
The required specialization and in general, more implementation level
details are described in the following sections.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataflow, Trigger and Event Builder}
\label{sec:fd-daq-hlt}

\metainfo{Giles Barr \& Josh Klein \& Giovanna Miotto \& Kurt Biery \& Brett Viren.  This is a DP/SP shared section.  It's file is \texttt{far-detector-generic/chapter-generic-daq/design-flow.tex}}

The data volume that is continuously flowing through the DUNE
\dword{sp} \dword{detmodule} DAQ from one APA is about \SI{10}{\GB/\s}
and from the entire far detector as much as \SI{10}{\TB/\s}.
However, by designing a distributed system that follows the natural
granularity of the detector, this high rate data is made manageable.
In particular the \dword{ringbuffer} is defined at the APA level for
the \dword{sp} \dword{detmodule}.
The majority of the data corresponds to the untriggered parts and
never leaves the computers hosting the \dwords{ringbuffer}.
The parts that exit do so in one of three circumstances: (a) as data
selected by nominal \dwords{trigcommand}, (b) as \dwords{trignote} or
(c) as previously triggered \dword{snb} candidate data which is
sent out after being dumped to the SSD storage local to
\dwords{rce} and as available bandwidth permits. 
All three of these transfers live entirely in the software domain of a
commodity computer farm and so a variety of techniques can be
considered for each; in this description, we consider each of them as
a data-pull protocol, similar to that in use at ProtoDUNE.

artDAQ is a modern, general-purpose software framework for event
processing. 
It has been used in the ProtoDUNE and other experiments.
It is optimized to expliot the parallelism that is possible with
modern multi-core machines. 
It is the principal architecture that will be used for the DUNE DAQ
back-end computing.
The authors of artDAQ have accomodated DUNE-specific feature-addition
requests and a number of libraries have been developed based on
existing parts of artDAQ used to handle incoming data from data
sources. 
It is likely that future DUNE extensions will be by one of these two
routes.

%(Section on FELIX data ingress, error handling, synchronisation,
%ring-buffer expiry). 
As noted above, one of the complex data flow control tasks is managing
the data as it enters the \dwords{ringbuffer} on the per-APA \dword{fe}
computing hosts. 
Data throughput on these nodes is governed by the use of the PCIe and
memory busses, and so the software will be written to mimumize
data-copies on the input side of the \dword{ringbuffer}.
%% removed: this is RCE-land, not primary buffer.
%During normal operation, there must be sufficient 'headroom' to allow
%an additional write of data to SSDs during the supernova burst. 
The current model for dataflow control is that each APA will operate
semi-autonomously, data being written directly from hardware into physical
memory and not moved. 
The software will keep track of the free and in-use memory (one way is
with a ring for each data source) and will maintain an index of the
data. 
One strategy that decentralises the error handling is for the writing
to the \dword{ringbuffer} to not cause error handling events, but
for it to flag areas of the data for which problems exist only if/when
it is to be read again. 
Similarly, to avoid memory alloction error handling, the incoming data
can overwrite the older data and read requests for data that has been
overwritten will report the error. 
It is planned to prototype the data transfers for the TDR.

% (Section on (a) event building)
Requests for event building
(i.e. reading data for succesful triggers from the \dwords{ringbuffer}
will be done using mostly standard features that are now in artDAQ. 
An \dword{eb} node is allocated for each trigger to be read and it
then requests portions of data from the \dwords{ringbuffer} for the
data from that event (this read must be initiated by the request, a
difference with the current ProtoDUNE).
If the data is missing (either due to errors when it was first
received, or because the request arrived so late that the data were
overwritten), the request still proceeds but with flags set in the event
header and which are displayed on the operators console.

% (Section on (b) trigger farming)
As described above, a \dword{fec} will receive \dwords{trigprimitive}
from its associated \dword{daqfer} which identify channels and periods
of time where potentially useful signal exists. 
In some circumstances, this local information will be sufficient to
form a \dword{trigcandidate} which on its own will lead to a
\dword{trigcommand}.
For example, when an interaction of sufficient energy occurs a
primitive will be generated for each nearby channel, the sum of the
primitive energies will pass some threshold and the union of their
times will be used construct a trigger command which covers a
bracketing time window. 
In the nominal design, trigger commands resulting in this way are
considered ``normal'' triggers. 
These commands are executed by one \dlong{eb} allocated from a pool. 
It queries the appropriate \dwords{daqds} for the period of requested
data, aggregates the returned data block across all \dwords{daqfrag},
includes the \dword{trigcommand} information itself and writes the
resulting \dword{rawevent} to the \dword{diskbuffer}.
Scaling methods are being considered to mitigate bottlenecks as may be
required.  
These are based on adding an intermediate layer which groups a number of
\dwords{fec} to avoid the possibility of too many small blocks of
data.

Another type of \dword{trigdecision} is performed which is more global
and happens when SNB neutrinos are interacting.
\Dwords{trigcandidate} from many \dwords{fec} must be combined across
the entire \dword{daqpart} to form an \dword{snb}
\dword{trigcommand}.
When an \dword{snb} command is raised in the \dword{mtl} it given to
the \dfirst{daqoob} for dispatching to the many \dwords{daqfer}.
This intermediary is used to allow the \dword{mtl} free to return to
processing any subsequent candidates and thus reduce trigger latency.

% (Section on (c) SNB trickling)
The average period between actual \dword{snb} to which DUNE is
sensitive is measured in decades. 
However, to maintain high efficiency to capture such important
physics, the thresholds will be placed as low as feasible, limited
only by the ability to acquire, validate and if passing, write out the
data to permanent storage. 
Even as such, the (largely false positive) \dword{snb} trigger rate is
expected to be low relative to normal triggers.
Understanding the exact rate requires more study, including using
early data, but for planning purposes it is taken to be once dump per
month on average.
Using the \dword{sp} \dword{detmodule} as an exemple, and choosing the
nominal time span for the dump to be \snbtime about \spsnbsize of
uncompressed data would result.
In the nominal \dword{sp} DAQ design, this dump would be spread over
600 \dword{ssd} units leading to \SI{75}{\GB} per SSD per dump.
% this number assumes defs.tex hasn't had its numbers changed.
% check once in a while
Thus, typical \dwords{ssd} offer storage to allow any given dump to be
held for at least one half year before it must be purged to assure
storage is available for subsequent dumps.
If every dump were to be sent to permanent storage, it would represent
a sustained \SI{0.14}{\Gbps} which is a small perturbation on the
bandwidth supplied throughout the DAQ network. 
Saved to permanent storage this would add \SI{0.5}{\PB/\year} which
would be substantial but a fraction of the total data budget.
The size of each dump is still larger than is convenient to place into
a single file and so the SNB event building will likely differ from
that for normal triggers in that the entire dump will not be held in a
single \dword{rawevent}.


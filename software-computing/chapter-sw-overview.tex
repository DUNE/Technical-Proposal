%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Where we describe the over arching model and boundaries of the
% computing and provide a guide to the organization of the volume
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Overview of Software and Computing }
\fixme{3 pages}

\section{Overview}

Offline computing for DUNE faces new and considerable challenges due to the large scale and diverse physics goals of the experiment.  In particular, the advent of Liquid Argon TPC's with exquisite resolution and sensitivity, combined with enormous physical volumes, creates challenges in acquiring and storing large data volumes and in analyzing and reducing them.  The computing landscape is changing rapidly, with the traditional HEP architecture of individual cores running linux being superseded by multi-core machines and CPU's. At the same time, algorithms for LAr reconstruction are still in their infancy and developing rapidly.  As a result we have reason to be optimistic about the future but are not able to predict it accurately.  The ProtoDUNE single and dual phase tests at CERN in the fall of 2018 will provide a wealth of data that will inform the future evolution of DUNE computing models.

The DUNE offline computing challenges can be classified in several ways.  We will start with the different detector/physics configurations that drive the large scale data storage and reconstruction. 
This discussion leans heavily on the DAQ design described in \ref{sec:fdsp-daq-des-consid}


\subsection{Physics challenges}

DUNE physics will consist of 

\begin{enumerate}
\item Long baseline neutrino oscillation studies, which require a near detector operating in a high rate environment and far detectors in which beam-coincident events are rare but in time with the accelerator and of sufficient energy to be readily recognizable.  

The proposed  full size 17 kT modules for the DUNE/LBNF\ref{FDsections} far detectors will  have an active volume 12m high, 14.5m wide and 58m long.  The first Single Phase(SP) module will have 
a total of 150 alternating vertical cathode and anode planes/module  spaced 3.5 m apart and operated at 180 kV for a 500 V/cm drift field.  The anode planes are made up of Anode Plane Assemblies  (APA's) which are 6.3 m tall by 2.3 m wide. %The APA's are wrapped with readout wires and read out through one of the shorter ends.  %Figure \ref{bigone} illustrates the layout of anode and cathode planes in the full size detector.

For modules of this size, drift times in the liquid argon are of order 2.5 msec and raw data sizes before compression are of order 25 GB per 5.4 msec readout window. Table \ref{sw:bigone} summarizes the estimates for 4 Single Phase modules from the 2015 Conceptual Design Report \cite{cdr-annex-rates}.  With no triggering and no zero suppression or compression, the raw event rates would be of order 145 EB/year. 

Requiring  coincidence with the LBNF beam will reduce the effective live-time from the full 1.2-1.5 sec beam spill to a 5.4 ms readout window leading to an untriggered data rate for beam-coincident events of around 20 GB/sec.

\fixme{Similar discussion of dual phase}

Requiring  coincidence with the LBNF beam will reduce the effective live-time from the full 1.2-1.5 sec beam spill to a 5.4 ms readout window leading to a data rate for beam-coincident events of around 20 GB/sec.
>>>>>>> 8f28b20950a4982236adee94594ef8f6f8ec20d4

\begin{table}[htp]
\begin{center}
%\begin{tabular}{|l|r|}
%\hline
%\# of samples/readout&10,800\\
%\# of channels&1,536,000\\
%readout window&5.4 ms\\
%SB readout window& 10 s\\
%bytes/full readout&24.9 GB\\
%\hline
%\end{tabular}
\begin{tabular}{|l|r|r|r|}
\hline
Process&rate&Full Size&Zero-suppressed\\
\hline
cosmic rate&0.259 Hz&24.9 GB&2.5 MB\\
beam triggers&$\sim 10,000$/year&24.9 GB&2.5 MB\\
$^{39}$Ar decays&11.2MHz&24.9 GB&$<$ 1 kB\\
Supernova candidates &12/year&46.1 TB&16.7 GB\\
\hline
%Estimated reconstruction time/event&600-2400 sec&
\end{tabular}
\end{center}
\caption{\normalsize \baselineskip 16 pt Estimated data rate parameters for 4 17kT Single Phase far detectors from the Conceptual Design Report. }
\label{bigone}
\end{table}%



Integrated over a year of 20M beam pulses, that leads to an uncompressed data size of 500 PB with compression giving a factor of 4-10 reduction. Some level of triggering will be needed to reduce this rate to levels that can be practically transferred and stored.



<<<<<<< HEAD
\item Rare physics processes, not in synchronization with the accelerator.  These include supernova physics, atmospheric neutrinos, proton decay, neutron conversion and solar neutrinos.  These processes are generally at lower energy, making triggering more difficult, and asynchronous, thus requiring an internal or external trigger.  In particular, supernovae signals will consist of a large number of low energy interactions spread throughout the far detector volume over a time period of 1-30 seconds. Buffering and storing 30 sec of data would require around 6000 readout windows, or around 120 TB/supernova readout.  

\item Near detector physics - the near detector configuration is not defined yet but we do have substantial experience from T2K and MicroBooNE at lower energies and MINERvA at the DUNE beam energies on cosmic and beam interactions under similar conditions.  We can expect that a near detector will experience 30-60 cosmic and beam interactions/beam pulse, spread over an area of a few square meters.  Storing and disentangling this information will be challenging but comparable to the ProtoDUNE data expected in 2018.

\end{enumerate}


\subsection{Relation to Trigger and DAQ}

The actual data volumes that will need to be recorded and reconstructed depend critically on the tradeoffs between complexity and cost in the detector and data acquisition systems and the cost of storing and reconstructing uninteresting data.

\subsubsection{Triggering}

If a decision can be made to read  out the detector based on internal information, data volumes can be substantially reduced.  
Challenges for triggering include electronic noise, differential response as a function of drift distance and radiological backgrounds.
Another major challenge will be the limited space and power availability at the far detector where critical electronics will need to be located underground in a tightly constrained environment.

\subsubsection{Lossless-Compression}

Lossless compression will substantially reduce the data rates at the cost of some CPU usage.  The exact degree of compression  depends on the electronic noise present and the detector occupancy.  Estimates for single phase are currently a factor of four and for dual phase, a factor of ten reduction.  ProtoDUNE experience will give us much better estimates on these factors.


\subsubsection{Zero suppression}

The data volumes discussed above are for un-suppressed, uncompressed data.  Efficient zero suppression mechanisms can substantially reduce the final data volume but previous experience in HEP indicates that this processes must be done carefully and often happens well into data-taking when the data are well understood.  Experience from MicroBooNE and the ProtoDUNE experiments will aid us in developing these algorithms but it is likely that they will be applied later in the processing chain.  

\subsubsection{Summary of boundary with Trigger/DAQ}

After discussion with the Trigger/DAQ group, we have agreed upon a feasible data transfer rate of 100 Gb/sec, which is consistent with projected network bandwidth from Sanford Lab to ESNET and 30 PB/year raw data stored to tape, which is substantial but within reasonable parameters for storage in the mid-2020's.  This will require some triggering and compression at the detector to achieve but allows both DAQ and Offline computing to proceed with reasonable design parameters.

Table \ref{daq:datarates} summarizes the data rates expected from the DAQ section of this proposal. 



\section{Building the computing model}\label{sw:bld-cmp-mdl}

The DUNE computing model is a work in progress.  We can expect that major advances will take place over the next year on several fronts, with data from ProtoDUNE and the full incorporation of lessons from MicroBooNE into LarSoft. 

<<<<<<< HEAD
The overall model can be divided into several major parts: Algorithms, Infrastructure and Adaption for the future.  These are in different stages of planning and completion

\begin{description}
\item{Algorithms}
This category includes the simulations, signal processing and reconstruction algorithms needed to reconstruct and understand our data. Algorithms are currently under development but informed by existing general codes (for example GENIE and GEANT4) and the experience of other liquid argon experiments as encoded in the shared larsoft project.
\item{Infrastructure}
This category includes the wealth of databases, catalogs, storage systems, compute farms and the software that drives them.  HEP fortunately has already developed much of this technology and our plan is to adopt preexisting systems where possible.  As DUNE is a fully global experiment, integrating the resources of multiple institutions is both an opportunity and a logistical challenge
\item{Adaptability}
As the experiment will be expected to run at least 2 decades past the present we must be prepared for the inevitable and major shifts in the underlying technologies that will occur. The ability to keep operating over decades almost requires that we emphasize open source over proprietary technologies for most applications.  We should also plan to be able to utilize and support a large range of compute architectures in order to fully utilize the resources available to the collaboration.
 \end{description}

In planning we have multiple valuable inputs.

\subsection{Core HEP infrastructure}
We plan to use shared HEP infrastructure wherever possible.  Notably the ROOT\cite{root} and GEANT4~\cite{geant4,Allison:2006ve} frameworks. 

\subsection{Running experiments}\label{sw:IF-input}

The Fermilab intensity frontier program experiments (MINOS\cite{minos}, MINERvA\cite{minerva} and NOvA\cite{nova} have developed substantial computing infrastructure for the storage, reconstruction and analysis of data on size scales of order 1/10 that of full DUNE and comparable to the ProtoDUNE experiments. While the LArTPC technology requires unique algorithms, the underlying compute systems, frameworks and database structures already exist and are being adapted for use on both ProtoDUNE and DUNE.

For algorithms, the MicroBooNE\cite{Acciarri:2016smi} experiment has been running since 2015 with a liquid Argon-TPC which shares many characteristics with the DUNE APA's.   MicroBooNE has, over the past year, published studies of noise sources and signal processing\cite{Acciarri:2017sde,Adams:2018dra}, novel pattern recognition strategies \cite{Acciarri:2016ryt,Acciarri:2017hat} and calibration signatures such as Michel electrons and cosmic rays \cite{Acciarri:2017sjy,Acciarri:2017sde}.  DUNE shares the both the \larsoft software framework and many expert collaborators with MicroBooNE and is taking direct advantage of their experience in developing simulations and reconstruction algorithms.


\subsection{ProtoDUNE}\label{sw:PD-planning}

The ProtoDUNE single and dual phase experiments will run in the Fall of 2018.  While the detectors themselves are 4\% of the final far detectors, the higher beam rates (up to 25 Hz) and presence of cosmic rays make the expected instantaneous data rates of 20 Gb/sec from these detectors comparable to those from the full far detectors and similar to those expected for a near detector. 
In addition, the entire suite issues in transferring, cataloging, reconstructing and analyzing these data are the same as for the full detectors and are driving the design and development of a substantial array of computing services necessary for both protoDUNE and DUNE.

Substantial progress is already being made on the infrastructure for computing, through a series of data challenges in late 2017 and early 2018 and are described in 
\fixme{Need section on data challenges}.  Development of reconstruction algorithms is currently restricted to simulation but is already informed by the experience with MicroBooNE data.


\section{Definition and scope of Software and Computing}

Testing new section reference style. See Section~\ref{sw:ov-intl-org}. This is Anne.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Boundaries with other experimental systems}
\subsection{Boundary with Data Acquisition}
\subsection{Boundary with Detector Controls and Monitoring}
\subsection{Boundary with Accelerator and Beam Operations}
\subsection{Boundary with Non-DUNE Experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{International Organization of Computing}
\label{sw:ov-intl-org}

\subsection{International Computing Centers}
\subsection{International Data Warehouses and Data Access}

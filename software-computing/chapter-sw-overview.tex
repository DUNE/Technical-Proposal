%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Where we describe the over arching model and boundaries of the
% computing and provide a guide to the organization of the volume
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Overview of Software and Computing }
\fixme{3 pages}

\section{Overview}

Offline computing for  \dword{dune} faces new and considerable challenges due to the large scale and diverse physics goals of the experiment.  In particular, the advent of Liquid Argon TPC's with exquisite resolution and sensitivity, combined with enormous physical volumes, creates challenges in acquiring and storing large data volumes and in analyzing and reducing them.  The computing landscape is changing rapidly, with the traditional HEP architecture of individual cores running linux being superseded by multi-core machines and CPU's. At the same time, algorithms for LAr reconstruction are still in their infancy and developing rapidly.  As a result we have reason to be optimistic about the future but are not able to predict it accurately.  The \dword{protodune} single and dual phase tests at CERN in the fall of 2018 will provide a wealth of data that will inform the future evolution of  \dword{dune} computing models.

The  \dword{dune} offline computing challenges can be classified in several ways.  We will start with the different detector/physics configurations that drive the large scale data storage and reconstruction. 
This discussion leans heavily on the \dword{daq} design described in \ref{sec:fdsp-daq-des-consid}


\subsection{Physics challenges}

 \dword{dune} physics will consist of 

\begin{enumerate}
\item Long baseline neutrino oscillation studies, which require a near detector operating in a high rate environment and far detectors in which beam-coincident events are rare but in time with the accelerator and of sufficient energy to be readily recognizable.  

The proposed  full size 17 kT modules for the  \dword{dune}/LBNF\ref{FDsections} far detectors will  have an active volume 12m high, 14.5m wide and 58m long.  The each Single Phase(SP) module will consist of
150 alternating vertical cathode and anode planes/module  spaced 3.5 m apart and operated at 180 kV for a 500 V/cm drift field.  The anode planes are made up of Anode Plane Assemblies  (APA's) which are 6.3 m tall by 2.3 m wide. %The APA's are wrapped with readout wires and read out through one of the shorter ends.  %Figure \ref{bigone} illustrates the layout of anode and cathode planes in the full size detector.
For modules of this size, drift times in the liquid argon are of order 2.5 msec and raw data sizes before compression are of order 25 GB per 5.4 msec readout window. Table \ref{sw:bigone} summarizes the estimates for 4 Single Phase modules from the 2015 Conceptual Design Report \cite{cdr-annex-rates}.  With no triggering and no zero suppression or compression, the raw data volume would be of order 145 exaB/year. 

Requiring  coincidence with the \dword{lbnf} beam will reduce the effective live-time from the full 1.2-1.5 sec beam cycle to a 5.4 ms readout window coincident with the 10 microsecond beam spill, leading to an uncompressed data rate for beam-coincident events of around 20 GB/sec.

Dual phase technology is likely to have lower data rates due to the lower number of readout channels. 
\fixme{more on DP}

%Requiring  coincidence with the \dword{lbnf} beam will reduce the effective live-time from the full 1.2-1.5 sec beam spill to a 5.4 ms readout window leading to a data rate for beam-coincident events of around 20 GB/sec.


\begin{table}[htp]
\begin{center}
%\begin{tabular}{|l|r|}
%\hline
%\# of samples/readout&10,800\\
%\# of channels&1,536,000\\
%readout window&5.4 ms\\
%SB readout window& 10 s\\
%bytes/full readout&24.9 GB\\
%\hline
%\end{tabular}
\begin{tabular}{|l|r|r|r|}
\hline
Process&rate&Full Size&Zero-suppressed\\
\hline
cosmic rate&0.259 Hz&24.9 GB&2.5 MB\\
beam triggers&$\sim 10,000$/year&24.9 GB&2.5 MB\\
$^{39}$Ar decays&11.2MHz&24.9 GB&$<$ 1 kB\\
Supernova candidates &12/year&46.1 TB&16.7 GB\\
\hline
%Estimated reconstruction time/event&600-2400 sec&
\end{tabular}
\end{center}
\caption{\normalsize \baselineskip 16 pt Estimated data rate parameters for 4 17kT Single Phase far detectors from the Conceptual Design Report. }
\label{sw:bigone}
\end{table}%



Integrated over a year of 20M beam pulses, that leads to an uncompressed data size of 500 PB.  Both lossless compression and triggering will be needed to further reduce this rate to levels that can be practically transferred and stored.




\item Processes not in synchronization with the accelerator.  These include supernova physics, atmospheric neutrinos, proton decay, neutron conversion and solar neutrinos.  These processes are generally at lower energy, making triggering more difficult, and asynchronous, thus requiring an internal or external trigger.  In particular, supernovae signals will consist of a large number of low energy interactions spread throughout the far detector volume over a time period of 1-30 seconds. Buffering and storing 30 sec of data would require around 6000 readout windows, or around 120 TB/supernova readout.  



\item Near detector physics - the near detector configuration is not yet defined  but we do have substantial experience from T2K and  \dword{microboone} at lower energies and  \dword{minerva} at the  \dword{dune} beam energies on cosmic and beam interactions under similar conditions.  We can expect that a near detector will experience 30-60 cosmic and beam interactions/beam pulse, spread over an area of a few square meters.  Storing and disentangling this information will be challenging but comparable to the \dword{protodune} data expected in 2018.



\item
{Calibration}
In addition to physics channels, continuous calibration of the detectors will be necessary.  It is likely that, for the far detectors, calibration dominate the data volume. Cosmic muons and atmospheric neutrino interactions will provide a substantial sample for energy and position calibration.  Dedicated runs with radioactive sources and laser calibration will also generate substantial and extremely valuable samples. Table \ref{daq:datarates} lists estimates for the single-phase far detector.  For the near detectors, calibration samples are likely to make up a smaller fraction of the final data sample.
\end{itemize}

Table \ref{daq:datarates} summarizes the event and data rates from the Single Phase Technical Proposal.

\fixme{Make certain this is the same as in the FD book }
\begin{dunetable}
[Anticipated event and data rates in the  \dword{dune} FD (from the single-phase chapter.]
{p{0.25\textwidth}p{0.15\textwidth}p{0.4\textwidth}}
{daq:datarates}
{Anticipated event and data rates in the  \dword{dune} FD assuming all four
  modules are single phase. The
  rates assume continuous readout (without any data reduction) for
  5.4~ms for non-extended events, and for 10 seconds for extended events.}   
Event Type  & Annual Data Volume & Assumptions \\ \toprowrule
 Beam interactions & 27 TB & 800 beam and 800 dirt muons; 10~MeV
 threshold in coincidence with beam time; include cosmics\\ \colhline
 Cosmics and atmospherics & 10 PB &  \\ \colhline
 Radiologicals & $\le$1~PB & fake rate of $\le$100 per year \cite{daq:simreport}\\ \colhline
 Front-end calibration & 200~TB & Four calibration runs per year, 100
 measurements per point \\ \colhline
 Radioactive source calibration & 100~TB & source rate $\le$10~Hz;
 single APA readout; lossless readout \\ \colhline
 Laser calibration & 200~TB & 1$\times$10$^6$ total laser
 pulses, lossy readout \\ \colhline
 Random triggers & 60 TB & 45 per day\\ \colhline
 Trigger primitives & $\le$6~PB &  all three wire planes; 12 bits per
 primitive word; 4 primitive quantities; $^{39}$Ar-dominated\\ \colhline
\end{dunetable}


\subsection{Relation to Trigger and \dword{daq}}

The actual data volumes that will need to be recorded and reconstructed depend critically on the tradeoffs between complexity and cost in the detector and data acquisition systems and the cost of storing, moving and reconstructing uninteresting data.

\subsubsection{Triggering}

If a decision can be made to read  out the detector based on internal information, data volumes can be substantially reduced.  
Challenges for triggering include electronic noise, differential response as a function of drift distance and radiological backgrounds.
Another major challenge will be the limited space and power availability at the far detector where critical electronics will need to be located underground in a tightly constrained environment.  This is the responsibility of the DAQ group with assistance from Software and Computing where needed. 

\subsubsection{Lossless-Compression}

Lossless compression will substantially reduce the data volume at the cost of some CPU time.  The exact degree of compression  depends on the electronic noise present and the detector occupancy.  Estimates for single phase are currently a factor of four and for dual phase, a factor of ten reduction.   \dword{protodune} experience will give us much better estimates on these factors.


\subsubsection{Zero suppression}

The data volumes discussed above are for unsuppressed, uncompressed data.  Efficient zero suppression mechanisms can substantially reduce the final data volume but previous experience in HEP indicates that signal processing must be done carefully and often happens well into data-taking when the data are well understood.  Experience from  \dword{microboone} and the \dword{protodune} experiments will aid us in developing these algorithms but it is likely that they will be applied later in the processing chain.  

\subsubsection{Summary of boundary with Trigger/\dword{daq}}

After discussion with the Trigger/DAQ group, we have agreed upon a feasible maximum data transfer rate of 100 Gb/sec, which is consistent with projected network bandwidth from Sanford Lab to ESNET and 30 PB/year raw data stored to tape.  This is substantial but within reasonable parameters for storage in the mid-2020's.  This will require some triggering and compression at the detector to achieve but allows both \dword{daq} and Offline computing to proceed with reasonable design parameters.

The constrained environment at the Sanford Lab motivates a model where further data reduction is done downstream, either on the surface or after delivery to computing facilities at FNAL or elsewhere. This could be analogous to the HLT's used by LHC experiments. The relative optimization of data movement and processing location is an important consideration for the design of both the DAQ and offline computing.

%Table \ref{daq:datarates} summarizes the data rates expected from the \dword{daq} section of this proposal. 



\section{Building the computing model}\label{sw:bld-cmp-mdl}

The  \dword{dune} computing model is a work in progress.  We can expect that major advances will take place over the next year on several fronts, with data from \dword{protodune} and the full incorporation of lessons from  \dword{microboone} into \dword{larsoft} . 


The overall model can be divided into several major parts:  Infrastructure, Algorithms and Adaption for the future.  These are in different stages of planning and completion.  An overarching theme is evaluating and using community codes and resources wherever possible. 



\subsection{Infrastructure}
This category includes the wealth of databases, catalogs, storage systems, compute farms and the software that drives them.  HEP fortunately has already developed much of this technology and our plan is to adopt preexisting systems wherever possible.  As  \dword{dune} is a fully global experiment, integrating the resources of multiple institutions is both an opportunity and a logistical challenge.

We are currently planning to have the primary data store at Fermilab.  For \dword{protodune}, data will also be stored at CERN.  Data processing is being designed to run on HEP grid resources with significant ongoing effort to containerize it so that we can make use of heterogenous resources worldwide. 


\subsubsection{Core HEP code infrastructure}
We plan to use shared HEP infrastructure wherever possible.  Notably the ROOT\cite{root} and GEANT4~\cite{geant4,Allison:2006ve} frameworks.   For event simulation, we plan to use the broad range of available generators (GENIE, NEUT, NuWro...) shared with the worldwide neutrino community.

In addition, we plan to use the infrastructure developed for the LHC and the Intensity Frontier experiments at Fermilab, notably grid infrastructure,  the \dword{Art} framework and the \dword{sam} data catalog.  The NOvA and MicroBooNe experiments are already using these tools for distributed computing and the ProtoDUNE data challenges are integrating CERN and Fermilab storage and CPU resources.  Our goal is to extend this integration to the  institutions within the collaboration who are capable of providing substantial storage and CPU resources. 



\subsection{Algorithms}
This category includes the simulations, signal processing and reconstruction algorithms needed to reconstruct and understand our data. Algorithms are currently under development but informed by existing general codes (for example GENIE and GEANT4) and the experience of other liquid argon experiments as encoded in the shared \dword{larsoft}  project.  Simulations are quite advanced but full understanding of reconstruction algorithms will need real data from ProtoDUNE. 





\subsection{Adaptability}
As the experiment will be expected to run at least 2 decades past the present we must be prepared for the inevitable and major shifts in the underlying technologies that will occur. The ability to keep operating over decades almost requires that we emphasize open source over proprietary technologies for most applications.  We should also plan to be able to utilize and support a large range of compute architectures in order to fully utilize the resources available to the collaboration.






\section{Planning inputs}


\subsection{Running experiments}\label{sw:IF-input}

The Fermilab intensity frontier program experiments (MINOS\cite{minos},  \dword{minerva}\cite{minerva} and  \dword{nova}\cite{nova} have developed substantial computing infrastructure for the storage, reconstruction and analysis of data on size scales of order 1/10 that of full  \dword{dune} and comparable to the \dword{protodune} experiments. While the LArTPC technology requires unique algorithms, the underlying compute systems, frameworks and database structures already exist and are being adapted for use on both \dword{protodune} and  \dword{dune}.

For algorithms, the  \dword{microboone}\cite{Acciarri:2016smi} experiment has been running since 2015 with a liquid Argon-TPC which shares many characteristics with the  \dword{dune} APA's.    \dword{microboone} has, over the past year, published studies of noise sources and signal processing\cite{Acciarri:2017sde,Adams:2018dra}, novel pattern recognition strategies \cite{Acciarri:2016ryt,Acciarri:2017hat} and calibration signatures such as Michel electrons and cosmic rays \cite{Acciarri:2017sjy,Acciarri:2017sde}.   \dword{dune} shares the both the \larsoft software framework and many expert collaborators with  \dword{microboone} and is taking direct advantage of their experience in developing simulations and reconstruction algorithms.


\subsection{ProtoDUNE}\label{sw:PD-planning}

The \dword{protodune} single and dual phase experiments will run in the Fall of 2018.  While the detectors themselves are 4\% of the final far detectors, the higher beam rates (up to 25 Hz) and presence of cosmic rays make the expected instantaneous data rates of 20 Gb/sec from these detectors comparable to those from the full far detectors and similar to those expected for a near detector. 

In addition, the entire suite of issues in transferring, cataloging, calibrating, reconstructing and analyzing these data are the same as for the full detectors and are driving the design and development of a substantial array of computing services necessary for both \dword{protodune} and  \dword{dune}.

Substantial progress is already being made on the infrastructure for computing, through a series of data challenges in late 2017 and early 2018. Development of reconstruction algorithms is currently restricted to simulation but is already informed by the experience with  \dword{microboone} data.

ProtoDUNE computing efforts are listed in Table \ref{sw:computingTasks}
\begin{dunetable}[Computing tasks]
{p{0.7\textwidth}}
{sw:computingTasks}
{Computing Tasks}
\\ Code management
\\ Documentation and logging of DAQ and detector configurations
\\ Data movement 
\\ Grid processing infrastructure
\\ Data catalog
\\ Beam instrumentation and databases
\\ Calibration and Alignment processing
\\ Calibration and Alignment databases
\\ Noise reduction
\\ Hit finding
\\ Pattern recognition algorithms
\\ Event simulation
\\ Analysis formats
\\ Distribution of analysis samples to collaborators
\end{dunetable}

All of these  are already in place or are in development for full ProtoDUNE data analysis and should carry over to the full DUNE.




\subsubsection{Single Phase prototype}

The single-phase prototype (ProtoDUNE-SP) will utilize six prototype APA's with the full drift length envisioned for the full far detector. In the single phase detector, the readout plane are immersed in the liquid Argon and no amplification occurs before the electronics.    ProtoDUNE-SP is being constructed in the NP04 beamline at CERN and should run with tagged test beam for around 6 weeks in the Fall of 2018.  In addition cosmic ray commissioning beforehand and cosmic running after the end of beam are anticipated.  Table \ref{tab:np04_data_rate}
shows the anticipated data rates and sizes. 



\begin{table}[htbp]
  \centering
  \begin{tabular}[h]{l|r}
\hline
    Trigger rate & 25\,Hz \\
    Spill duration & 4.5\,s\\
    SPS Cycle & 22.5\,s \\
    Readout time window & 5\,ms \\
    \# of APAs to be read out & 6 \\
    \hline
    Single readout size (per trigger) & 230.4\,MB \\
    Lossless compression factor & 4 \\
    Instantaneous data rate (in-spill) & 1440\,MB/s \\
    Average data rate & 576\,MB/s \\
    \hline
    3-Day buffer depth & 300\,TB \\
    \hline
  \end{tabular}
  \caption{Parameters for ProtoDUNE-SP run at CERN including both
  the in-spill and out-of-spill data.}
  \label{tab:np04_data_rate}
\end{table}
\subsection{Dual-Phase prototype}

The dual-phase prototype will run in the NP02 beamline in late Fall 2018.  In this detector, electrons drift the full height of the cryostat, emerge from the liquid and are collected - after gas amplification - on an grid of instrumented pads at the top of the detector.  The WA105 3x1x1 m test of this technology ran successfully in the summer of 2017\cite{Murphy:20170516}.  Table \ref{tab:np02_data_rate} shows the expected data rates from ProtoDUNE-DP. 

\begin{table}[htbp]
  \centering
  \begin{tabular}[h]{l|r}
\hline
     Compressed, non-zero-suppressed event size & 15.9 MB\\
    Average beam data flow to offline data storage &   305.3 MB/s  \\
    Average off-spills cosmic data flow to offline data storage &   12.4 MB/s\\
    Planned total statistics of beam triggers in 30 beam days with 50\% efficiency&25M\\
    Planned overall storage size of beam events&   0.4 PB\\
   Cosmic rays in 30  beam days&  2.1  M\\
   Planned storage size for cosmic rays during beam period&  0.04 PB\\
   Requested (in 2016) overall storage envelope for ProtoDUNE-DP&0.7 PB \\
    \hline
  \end{tabular}
 \caption{NP02 data volume}
  \label{tab:np02_data_rate}
\end{table}

\subsection{Data Challenges}

Computing and software is performing a series of data challenges to ensure that systems will be ready when the detectors become fully operational in the summer of 2018.  To date we have performed challenges using simulated single-phase data with a joint dual/single phase test scheduled for April of 2018.

In data challenge 1.5 in mid-January, dummy data based on non-zero-suppressed simulated events were produced at EHN1 and successfully transferred via 10-50 parallel transfers to the EOS disk systems in the CERN Tier-0 data center at a sustained rate of 2GB/sec.    Transfers to dCache/Enstore at Fermilab achieved rates of 500 MB/sec.  
The rate from EHN1 to EOS is already close to the maximum expected during ProtoDUNE-SP running.

\subsubsection{Reconstruction tests}
Reconstruction tests have been performed on simulated test beam interactions with cosmic rays and an electronic noise simulation based on MicroBooNE's experience.  Hit finding and shaping is found to take around 2 minutes/event with a 2GB memory footprint, leading to a reduction in data size of a factor of four.  Higher level pattern recognition occupies 10 minutes/event with a 4-6GB memory footprint. For real data, calibration, electric field non-uniformities and other factors will likely raise the CPU needs per event. We will learn this when real data starts to arrive in late summer. 


\section{Resource planning}



The  \dword{dune} computing effort  relies heavily on the human and hardware resources of  multiple organizations,  with the bulk of hardware resources at CERN and Fermilab.  The   \dword{dune} computing organization serves as an interface between the collaboration and those organizations.  Computational resources are generally negotiated on a yearly basis, with additional resources available opportunistically. Human assistance is largely on a per-project  basis, with substantial support when needed but very few personnel permanently assigned to the  \dword{dune} or \dword{protodune} efforts.

The \dword{protodune} efforts in 2018-2019 will exercise almost all computing aspects of DUNE, although at smaller scale.  Much of the infrastructure needed for full DUNE, in particular  databases, grid configurations and code management systems will need to be developed over the next year, with final algorithms and full scale data movement and processing systems coming later when needed. 


%
%\section{Definition and scope of Software and Computing}
%
%Testing new section reference style. See Section~\ref{sw:ov-intl-org}. This is Anne.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Boundaries with other experimental systems}
%\subsection{Boundary with Data Acquisition}
%\subsection{Boundary with Detector Controls and Monitoring}
%\subsection{Boundary with Accelerator and Beam Operations}
%\subsection{Boundaries with CERN and Fermilab computing organizations}
%The  \dword{dune} computing effort  relies heavily on the human and hardware resources of  multiple organizations,  with the bulk of hardware resources at CERN and Fermilab.  The   \dword{dune} computing organization serves as an interface between the collaboration and those organizations.  Computational resources are generally negotiated on a yearly basis, with additional resources available opportunistically. Human assistance is largely on a per-project  basis, with substantial support when needed but very few personnel permanently assigned to the  \dword{dune} or \dword{protodune} efforts. 
%\subsection{Boundary with Non- \dword{dune} Experiments}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{International Organization of Computing}
%\label{sw:ov-intl-org}
%
%\subsection{International Computing Centers}
%\subsection{International Data Warehouses and Data Access}
